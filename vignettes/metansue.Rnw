%\documentclass[article]{jss}
\documentclass[nojss]{jss}
%\VignetteIndexEntry{Meta-analysis of Non Statistically-significant Unreported Effects (MetaNSUE)}
\usepackage{amsmath}
\usepackage{supertabular}
\usepackage{enumitem}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{bigints}
\lstset{basicstyle=\footnotesize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% declarations for jss.cls %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% almost as usual
\author{Anton Albajes-Eizagirre\\\tiny{FIDMAG Germanes Hospital\`{a}ries} \\\tiny{Mental Health Research Networking Center (CIBERSAM)}\And 
        Aleix Solanes\\\tiny{FIDMAG Germanes Hospital\`{a}ries} \\\tiny{CIBERSAM}\And Joaquim Radua\\\tiny{FIDMAG Germanes Hospital\`{a}ries} \\\tiny{CIBERSAM}\\ \tiny{Centre for Psychiatric Research and Education, KI}\\\tiny{Institute of Psychiatry, Psychology and Neuroscience, KCL}   }
\title{Meta-analysis of Non Statistically-significant Unreported Effects (MetaNSUE), explained and revisited}

%% for pretty printing and a nice hypersummary also set:
\Plainauthor{Anton Albajes-Eizagirre, Aleix Solanes, Joaquim Radua} %% comma-separated
\Plaintitle{Meta-analysis of Non Statistically-significant Unreported Effects (MetaNSUE), explained and revisited} %% without formatting
\Shorttitle{\pkg{MetaNSUE}: Revisited Meta-analysis of Non Statistically-significant Unreported Effects} %% a short title (if necessary)

%% an abstract and keywords
\Abstract{
This introduction to the \proglang{R} package \pkg{metansue} is a (slightly)
modified version of a text submitted to the \emph{Journal of Statistical Software}.

Published studies sometimes report that a difference or correlation was not statistically significant but do not report its effect size, or any statistic from which the latter may be derived. Unfortunately, these studies cannot be excluded from meta-analyses, because their exclusion would strongly bias the meta-analytic outcome, but can neither be included as null effect sizes because this strategy is also associated to bias. To overcome this problem we have previously developed and validated MetaNSUE, a novel method based on multiple imputations of the missing information. Here we present an updated \proglang{R} package with several numerical improvements to make the method more robust in extreme scenarios, as well as an easy-to-use Graphical User Interface for non-\proglang{R} users. We also include examples of its use.}
\Keywords{Meta-analysis, missing, non-reported, bias, correlation, standardized mean difference, standardized mean change, GUI, \proglang{R}}
\Plainkeywords{meta-analysis, R} %% without formatting
%% at least one keyword must be supplied

%% publication information
%% NOTE: Typically, this can be left commented and will be filled out by the technical editor
%% \Volume{50}
%% \Issue{9}
%% \Month{June}
%% \Year{2012}
%% \Submitdate{2012-06-04}
%% \Acceptdate{2012-06-04}

%% The address of (at least) one author should be given
%% in the following format:
\Address{
  Anton Albajes-Eizagirre\newline
  FIDMAG Germanes Hospital\`{a}ries\\
  Carrer del Dr. Antoni Pujadas, 38\\
  08830 Sant Boi de Llobregat, Catalonia (Spain)\\
  \textit{and}\\
  Mental Health Research Networking Center (CIBERSAM)\\
  Barcelona, Catalonia (Spain)\\
  E-mail: \email{aalbajes@fidmag.com}\\
  \newline
  Aleix Solanes \newline
  FIDMAG Germanes Hospital\`{a}ries\\
  Carrer del Dr. Antoni Pujadas, 38\\
  08830 Sant Boi de Llobregat, Catalonia (Spain)\\
  \textit{and}\\
  Mental Health Research Networking Center (CIBERSAM)\\
  Barcelona, Catalonia (Spain)\\
  E-mail: \email{asolanes@fidmag.com} \newline
  \newline
  Joaquim Radua\\
  FIDMAG Germanes Hospital\`{a}ries\\
  Carrer del Dr. Antoni Pujadas, 38\\
  08830 Sant Boi de Llobregat, Catalonia (Spain)\\
  \textit{and}\\
  Mental Health Research Networking Center (CIBERSAM)\\
  Barcelona, Catalonia (Spain)\\
  \textit{and}\\
  Centre for Psychiatric Research and Education\\
  Department of Clinical Neuroscience\\
  Karolinska Institutet, Stockholm, Sweden\\
  \textit{and}\\
  Department of Psychosis Studies\\
  Institute of Psychiatry, Psychology and Neuroscience\\
  King's College London, London, UK\\
  E-mail: \email{jradua@fidmag.com}\\
  URL: \url{http://www.radua.net/}
%  URL: \url{http://eeecon.uibk.ac.at/~zeileis/}
}


%% It is also possible to add a telephone and fax number
%% before the e-mail in the following format:
%% Telephone: +43/512/507-7103
%% Fax: +43/512/507-2851

%% for those who use Sweave please include the following line (with % symbols):
%% need no \usepackage{Sweave.sty}

%% end of declarations %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}

%% include your article here, just as usual
%% Note that you should use the \pkg{}, \proglang{} and \code{} commands.

\section[Introduction]{Introduction}

When conducting a meta-analysis, researchers may find that some studies report that the effect (e.g., a difference between two groups) was not statistically significant but do not report the specific value of the outcome, or any statistic from which the outcome may be derived (e.g., a t-value). These non-statistically-significant unreported effects (NSUE) cannot be excluded from the meta-analysis but can neither be included assuming them to be null. Note that they probably correspond to small effect sizes, and thus its exclusion would bias the meta-analysis upwards. However, they are not probably null, for what including them as null effect sizes would bias the meta-analysis downwards (\cite{RN1}).\\

To correctly include studies with NSUEs into meta-analyses, we developed MetaNSUE, a novel method based on multiple imputations algorithms (\cite{RN2}). Briefly, the method consists in obtaining maximum likelihood estimations (MLE) of the missing outcomes (i.e., the NSUEs), adding noise to these estimations to multiply impute the NSUEs, conducting a meta-analysis for each set of imputations, and finally pooling these meta-analyses. Importantly, the bounds of statistical significance of the NSUEs are taken into account throughout the analyses.

In its empirical validation, we showed that estimations were strongly biased when conducting a standard meta-analysis without NSUEs, moderately negatively biased when converting NSUEs to zeros, and nearly unbiased when using MetaNSUE (\cite{RN2}). However, in subsequent uses we have detected that the method may be unstable in two extreme scenarios, namely in meta-analyses with seldom known data, or when studies use unconventional bounds (e.g., 9.9 $<$ z $<$ 10). Relevantly, these scenarios are indeed common in some disciplines such as voxel-based neuroimaging.\\

The aim of this paper is two-fold. First, we detail a series of numerical modifications in the original implementation of the software to make the MLE substantially more robust in extreme scenarios. Second, we describe the \pkg{MetaNSUE} package for \proglang{R} as well as its new Graphical User Interface (GUI), freely available at \href{http://www.metansue.com/}{www.metansue.com}, \href{http://www.sdmproject.com/}{www.sdmproject.com} and at the CRAN repository. 

\section{Conversion of the raw statistics to effect sizes}
%\subsection{Derivation of effect sizes and their variances}
Prior to the MLE step, known statistics (e.g., t-values) must be converted to effect sizes using standard formula (\cite{RN1}).
The conversion is conducted with simple \proglang{R} functions, specific for each measure, that return an object of class ``nsue'':
%% Note: If there is markup in \(sub)section, then it has to be escape as above.
\begin{itemize}
\item{}``smc\_from\_t'' converts one-sample $t$-values into standard mean changes:
  \begin{equation}
y_{i} =J\left(df_{i} \right)\cdot \sqrt{\frac{1}{n_{i} } } \cdot t_{i} 
  \end{equation}  
  where $J$ is the exact form of the Hedge correction factor (\cite{RN3}), $df_i$ are the degrees of freedom, $n_i$ is the sample size and $t_i$ is the $t$-value. To note, $J$ has been numerically improved for the revisited version of the method for large degrees of freedom using the combination of the \proglang{R} functions ``exp'' and ``lgamma'' instead of the function ``gamma''.
\item{}``smd\_from\_t'' converts two-sample $t$-values into standard mean differences:
  \begin{equation}
    y_{i} =J\left(df_{i} \right)\cdot \sqrt{\frac{1}{n_{i,1} } +\frac{1}{n_{i,2} } } \cdot t_{i}
  \end{equation}  
  where $n_{i,1}$ is the size of the first group, and $n_{i,2}$ is the size of the second group. 
\item{} ``z\_from\_r'' converts $r$ correlation coefficients into $z$ values using the Fisher transform):
  \begin{equation}
    y_{i} ={\rm atanh}(r_{i} )
  \end{equation}
  where $r_i$ is the correlation coefficient.
\end{itemize}
Statistics in studies with NSUEs must be coded with ``NA'', so that the above functions know that these studies have NSUEs and use the alpha level of the studies to calculate the effect sizes corresponding to the bounds of non-statistical significance (i.e., the unknown effect-sizes must be within these bounds; otherwise, they would have reached statistical significance).\\

The variances of correlations only depend on sample sizes and thus can be straightforwardly derived:
\begin{equation}
  v_{i} =\frac{1}{n_{i} -3}
\end{equation}
Conversely, the variances of standardized mean changes and differences depend on the effect sizes, which are unknown in NSUE studies. To overcome this problem, the formula of the variance is expressed as a function of two constants ($k_{1}$ and $k_{2}$) that only depend on the sample sizes (or the degrees of freedom) but allow a subsequent straightforward derivation of the variances:
\begin{equation}
  \begin{aligned}
  v_{i} &= k_{1,i} +k_{2,i} \cdot y_{i}^{2} \\
  k_{1,i} &=\left\{\begin{array}{cc} {{\rm smc:}} & {\frac{1}{n_{i} } } \\ {{\rm smd:}} & {\frac{1}{n_{i,1} } +\frac{1}{n_{i,2} } } \end{array}\right. \\
  k_{2,i} &=1-\frac{df_{i} -2}{df_{i} \cdot J(df_{i} )^{2} }
  \end{aligned}
\end{equation}
\subsection{Study labels and correlation}
Objects of class ``nsue'' also contain the labels of the studies, which beyond being useful for associating data to studies (e.g., in a forest plot), are used to indicate which studies are related (MetaNSUE understands that all studies with the same label are related). The expected correlation between dependent studies can also be specified. 
\subsection{Example of use}
Before we start with the examples, the metansue package has to be loaded:
<<prompt = "R> ", continue = "+  ", width = 70, useFancyQuotes = FALSE>>=
library(metansue)
@
Then we can run the example:
<<prompt = "R> ", continue = "+  ", width = 70, useFancyQuotes = FALSE>>=
t <- c(3.4, NA, NA, NA, NA, 2.8, 2.1, 3.1, 2.0, 3.4)
n <- c(40, 20, 22, 24, 18, 30, 25, 30, 16, 22)
x <- smc_from_t(t, n)
x
str(x)
@

\subsection{Special case: missing correlations in two-sample pre-post studies}
Two-sample pre-post studies can be meta-analyzed with the \proglang{R} functions ``smd\_from\_t'' and ``meta'' (see below), i.e., imputing the unknown effect sizes of the between-group comparison of the pre-post effects. However, an alternative method consists in imputing the unknown correlations between pre and post measures, and using them, along with the reported means and standard deviations of the groups at the two time-points, to estimate the unknown effect sizes of the between-group comparison of the pre-post differences \cite{RN17}. An advantage of this method may be that imputed data is ensured to follow the pre-post correlations. However, is inapplicable when means and standard deviations are not fully reported.

To apply this method, user can use the \proglang{R} function ``r\_in\_smd\_from\_t\_means\_and\_sds1'', which calculates the (Fisher transform of the) pre-post correlation:
\begin{equation}
  \label{eq:vpooled}
  \begin{aligned}
    v_{pooled}  &= \frac{df_{1} \cdot v_{1} +df_{2} \cdot v_{2} }{df_{1} +df_{2} }  \\
        &= \frac{df_{1} \cdot \left(v_{1,pre} +v_{1,post} -2\cdot s_{1,pre} \cdot s_{1,post} \cdot r\right)+df_{2} \cdot \left(v_{2,pre} +v_{2,post} -2\cdot s_{2,pre} \cdot s_{2,post} \cdot r\right)}{df_{1} +df_{2} } \\
  r&=\frac{df_{1} \cdot \left(v_{1,pre} +v_{1,post} -v_{pooled} \right)+df_{2} \cdot \left(v_{2,pre} +v_{2,post}^{2} -v_{pooled} \right)}{2\cdot \left(df_{1} \cdot s_{1,pre} \cdot s_{1,post} +df_{2} \cdot s_{2,pre} \cdot s_{2,post} \right)}
  \end{aligned}
  \end{equation}
where, $m_{j}$, $s_{j}$, $v_{j}$, $df_{j}$ are the mean, standard deviation, variance and degrees of freedom of group $j$, $v_{pooled}$ is the pooled variance (straightforwardly derived from $t$-values) and $r$ is the pre-post correlation.

Correlations can be then imputed and meta-analyzed using the \proglang{R} function ``meta''. This meta-analysis is of no interest but we recommend that the user checks, e.g., that correlation values are plausible.

Finally, imputed correlations may be converted back to between-group comparisons of the pre-post differences (see \eqref{eq:vpooled}) using the \proglang{R} function ``r\_in\_smd\_from\_t\_means\_and\_sds2''. This function also calls again the last part of the \proglang{R} function ``meta'', easily returning the meta-analysis of interest.
 
Example of use:
<<prompt = "R> ", continue = "+  ", width = 70, useFancyQuotes = FALSE>>=
t <- c(NA, -2.2, -3.0, -2.1, NA, -2.7, -2.6, -2.4, -3.2)
n1 <- c(18, 15, 22, 17, 14, 23, 29, 24, 45)
mean1.pre <- c(49.9, 49.9, 50.2, 49.8, 50.1, 50.0, 49.9, 51.0, 49.8)
sd1.pre <- c(5.1, 5.2, 6.6, 5.3, 5.5, 3.6, 3.2, 5.9, 5.9)
mean1.post <- c(65.6, 65.7, 66.0, 65.8, 66.6, 65.9, 66.0, 67.0, 65.9)
sd1.post <- c(6.1, 7.3, 5.4, 6.1, 4.6, 7.1, 4.6, 2.8, 7.1)
n2 <- c(18, 18, 23, 18, 16, 23, 27, 23, 45)
mean2.pre <- c(50.8, 50.2, 50.0, 49.7, 49.8, 50.6, 49.8, 50.9, 49.9)
sd2.pre <- c(5.2, 5.9, 4.0, 7.5, 5.7, 4.0, 5.8, 5.6, 5.4)
mean2.post <- c(69.9, 70.3, 70.4, 69.8, 70.2, 70.0, 69.7, 70.8, 69.7)
sd2.post <- c(5.2, 4.7, 4.5, 3.6, 3.7, 6.1, 4.6, 5.9, 6.2)
r <- r_in_smd_from_t_means_and_sds1(t, n1, mean1.pre, sd1.pre,
     mean1.post, sd1.post, n2, mean2.pre, sd2.pre, mean2.post, sd2.post)
r
mr <- meta(r)
mr
m <- r_in_smd_from_t_means_and_sds2(mr)
m
@

\section{NSUE meta-analysis}
The command ``meta'' conducts the core steps of the analysis:
\begin{enumerate}[label=\alph*)]
\item MLE of the unknown effect-sizes and between-study heterogeneity (\cite{RN2})
\item Multiple imputation of these unknown effect sizes based on the MLE (\cite{RN4})
\item Separate meta-analyses for each set of imputations (\cite{RN5})
\item Pooling of the results of the meta-analyses (\cite{RN6})
\item Hypothesis testing using the pooling statistics (\cite{RN7})
\end{enumerate}

\subsection{Maximum likelihood estimation (MLE)}
It must be highlighted that the MLE is conducted with the only aim to provide a basis for the subsequent multiple imputations of the unknown effects. Numbers resulting from this step cannot be understood as meta-analytic results, because they do not incorporate the complex body of variances involved in a meta-analysis. These variances are not only important per se, but also for the estimation of the effect sizes.
\subsubsection{Weight for related studies}
As in a general linear model, the researcher could deal with related studies by specifying the correct factors in the design matrix. However, a simpler and more convenient approach in some meta-analyses is to combine each group of related studies into a single ``combined study'' with decreased variance (\cite{RN8}\cite{RN9}\cite{RN10}). To this end, MetaNSUE calculates a MLE weight for each study according to the following formula (\cite{RN2}):
\begin{equation}
  \label{eq:weightrelated}
w_{i} =\frac{1}{1+\left(R_{i} -1\right)\cdot r}
\end{equation}
where $R_{i}$ is the number of studies with the same label as the $i^{th}$ study and $r$ is the expected correlation specified in the ``nsue'' object.
Note that with this adjustment, the overall weight of a group of related studies is:
\begin{equation}
R_{i} \cdot w_{i} =\frac{R_{i} }{1+\left(R_{i} -1\right)\cdot r}
\end{equation}
Note that $R_{i} \cdot w_{i} = 1$ if $r_{r}=1$ (i.e., the related studies are indeed the same), $R_{i} \cdot w_{i} = R_{i}$ if $r_{r}=0$ (i.e., the ``related'' studies are indeed unrelated), and $1 < R_{i} \cdot w_{i} < R_i$ if $r$ lies between 0 and 1.
\subsubsection{The log-likelihood}
MLE is based on finding the parameters that maximize the likelihood of the data. Specifically, the likelihood to maximize is the multiplication of the likelihood of each reported effect size (obtained with the probability density function, $pdf$) and the likelihood that each unreported effect size lays within its two effect size bounds (obtained with the probability mass function, $pmf$):
\begin{equation}
  \label{eq:loglikelihood}
  \begin{aligned}
    L\left(\theta ;y_{i} ,\ldots ;v_{i}^{2} ,\ldots ;w_{i} ,\ldots ;y_{{\alpha  \mathord{\left/{\vphantom{\alpha  2}}\right.\kern-\nulldelimiterspace} 2} ,j} ,y_{{1-\alpha  \mathord{\left/{\vphantom{1-\alpha  2}}\right.\kern-\nulldelimiterspace} 2} ,j} ,\ldots ;v_{{\alpha  \mathord{\left/{\vphantom{\alpha  2}}\right.\kern-\nulldelimiterspace} 2} ,j}^{2} ,v_{{1-\alpha  \mathord{\left/{\vphantom{1-\alpha  2}}\right.\kern-\nulldelimiterspace} 2} ,j}^{2} ,\ldots ;w_{j} ,\ldots \right) \\
    =\prod _{i=1}^{N_{1} }pdf\left(y_{i} ;v_{i}^{2} |\theta \right)^{w_{i} }  \cdot \prod _{j=1}^{N_{2} }pmf\left(y_{{\alpha  \mathord{\left/{\vphantom{\alpha  2}}\right.\kern-\nulldelimiterspace} 2} ,j} ,y_{1-{\alpha  \mathord{\left/{\vphantom{\alpha  2}}\right.\kern-\nulldelimiterspace} 2} ,j} ;v_{{\alpha  \mathord{\left/{\vphantom{\alpha  2}}\right.\kern-\nulldelimiterspace} 2} ,j}^{2} ,v_{1-{\alpha  \mathord{\left/{\vphantom{\alpha  2}}\right.\kern-\nulldelimiterspace} 2} ,j}^{2} |\theta \right)^{w_{j} }
  \end{aligned}
\end{equation}
where $\theta$ are the parameters to be estimated, $y$ are effect sizes, $v^{2}$ their variances, $N_{1}$ is the number of reported effect sizes, $N_{2}$ is the number of studies with NSUEs.

The parameters to estimate include not only the coefficients of interest (usually the intercept, i.e., the main outcome of a meta-analysis) but also regressors that may be used to better estimate the effect size of a study. The original version of MetaNSUE also included the between-study heterogeneity $\tau^{2}$ as a parameter to estimate, but the revisited version conducts this estimation in a subsequent step due to the modifications described in the next section.

Assuming normality, the $pdf$ is simply:

\begin{equation}
  \label{eq:pdf}
  pdf\left(y_{i} ,v_{i}^{2} |\beta \right)=\frac{1}{v_{i} } \phi \left(\frac{y_{i} -X_{i} \cdot \beta }{v_{i} } \right)=\frac{1}{v_{i} } \cdot \frac{1}{\sqrt{2\pi } } \cdot \exp \left(-\frac{1}{2} \cdot \left(\frac{y_{i} -X_{i} \cdot \beta }{v_{i} } \right)^{2} \right)
\end{equation}
where $X_{i}$ is the $i^{th}$ row of the design matrix of the model of covariates, $\beta$ is the vector of coefficients of the model of covariates, and $\phi$ is the probability density function of the standard normal distribution.

The $pmf$ is the difference of the cumulated distribution function evaluated at the upper and lower effect size bounds:

\begin{equation}
  \label{eq:pmf}
  pmf\left(y_{{\alpha  \mathord{\left/{\vphantom{\alpha  2}}\right.\kern-\nulldelimiterspace} 2} ,i} ,y_{1-{\alpha  \mathord{\left/{\vphantom{\alpha  2,i}}\right.\kern-\nulldelimiterspace} 2,i} } ;v_{{\alpha  \mathord{\left/{\vphantom{1-\alpha  2,i}}\right.\kern-\nulldelimiterspace} 2,i} }^{2} ,v_{{1-\alpha  \mathord{\left/{\vphantom{\alpha  2,i}}\right.\kern-\nulldelimiterspace} 2,i} }^{2} |\beta \right)=\Phi \left(\frac{y_{{1-\alpha  \mathord{\left/{\vphantom{1-\alpha  2,i}}\right.\kern-\nulldelimiterspace} 2,i} } -X_{i} \cdot \beta }{v_{{1-\alpha  \mathord{\left/{\vphantom{1-\alpha  2,i}}\right.\kern-\nulldelimiterspace} 2,i} } } \right)-\Phi \left(\frac{y_{{\alpha  \mathord{\left/{\vphantom{\alpha  2,i}}\right.\kern-\nulldelimiterspace} 2,i} } -X_{i} \cdot \beta }{v_{{\alpha  \mathord{\left/{\vphantom{\alpha  2,i}}\right.\kern-\nulldelimiterspace} 2,i} } } \right)
\end{equation}
where $\Phi$ is the cumulated distribution function of the standard normal distribution.

In practice it is more convenient to minimize the minus log-likelihood than to directly maximize the likelihood. Thus, the expression to minimize is the sum of the minus logarithms of the $pdf$ or $pmf$ of each study. 

The minus logarithm of the $pdf$ is again relatively simple:

\begin{equation}
  \label{eq:logpdf}
  \begin{aligned}
    -log\left(pdf\left(y_{i} ,v_{i}^{2} |\beta \right)\right) &= \log \left(v_{i} \right)+\frac{1}{2} \cdot \log \left(2\pi \right)+\frac{1}{2} \cdot \frac{\left(y_{i} -X_{i} \cdot \beta \right)^{2} }{v_{i}^{2} }
    \\
    &= \frac{1}{2} \cdot \log \left(2\pi \right)+\frac{1}{2} \cdot \log \left(v_{i}^{2} \right)+\frac{1}{2} \cdot \frac{\left(y_{i} -X_{i} \cdot \beta \right)^{2} }{v_{i}^{2} } \\
    &= (ct.)+\frac{1}{2} \cdot \left[\log \left(v_{i}^{2} \right)+\frac{\left(y_{i} -X_{i} \cdot \beta \right)^{2} }{v_{i}^{2} } \right]
    \end{aligned}
\end{equation}
Conversely, the logarithm of the $pmf$ may be numerically unstable in an extreme scenario when the two expressions inside the cumulated distributed are large and close. To overcome numerical errors, the revisited version of MetaNSUE includes the following transformation:
\begin{equation}
  \label{eq:logpmf}
  \begin{aligned}
    \log \left(\Phi \left(m\right)-\Phi \left(n\right)\right)
    &=\log \left(\Phi \left(m\right)\cdot \left(1-\frac{\Phi \left(n\right)}{\Phi \left(m\right)} \right)\right)\\
    &=\log \left(\Phi\left(m\right)\right)+\log \left(1-\frac{\Phi \left(n\right)}{\Phi \left(m\right)} \right) \\
    &=\Phi _{\log } \left(m\right)+\log \left(-\left(\frac{\Phi \left(n\right)}{\Phi \left(m\right)} -1\right)\right)\\
    &=\Phi _{\log } \left(m\right)+\log \left(-\left(\exp \left(\log \left(\frac{\Phi \left(n\right)}{\Phi \left(m\right)} \right)\right)-1\right)\right)\\
    &=\Phi _{\log } \left(m\right)+\log \left(-\left(\exp \left(\log \left(\Phi \left(n\right)\right)-\log\left(\Phi\left(m\right)\right)\right)-1\right)\right)\\
    &=\Phi _{\log } \left(m\right)+\log \left(-\left(\exp \left(\Phi _{\log } \left(n\right)-\Phi _{\log } \left(m\right)\right)-1\right)\right)\\
    &=\Phi _{\log } \left(m\right)+\log \left({\rm -expm1}\left(\Phi _{\log } \left(n\right)-\Phi _{\log } \left(m\right)\right)\right)
  \end{aligned}
\end{equation}
where expm1$(x)$ is equivalent to $e^x-1$ but with increased numerical accuracy. For $m = 10$ and $n = 9.9$, the initial expression wrongly returns $-\infty$, whereas the last, equivalent expression correctly returns $-52.7$.

Minimization of the minus log-likelihood is conducted using the \proglang{R} function ``optimize'' when there is a single coefficient and with the \proglang{R} function ``optim'' when there are two or more coefficients. The latter function requires initial values for the parameters, which in the original version were simply the mean for the intercept and zeroes for the remaining coefficients. In the revisited version, these initial values are better estimated with a fixed-effects meta-analysis (assuming NSUEs to be the mean of their upper and lower bounds).
\subsubsection{Leave-one-out protection}
When nearly all studies of a meta-analysis are NSUEs, there is a possibility that the single or few studies with known measures drive the MLE, potentially leading to a falsely positive meta-analysis. This situation may be detected in a subsequent leave-one-out jack-knife analysis when the artifact is clearly produced by a single study. For example, this was the case for reward feedback in \cite{RN2}. However, the issue could potentially be unnoticed in less extreme situations.

To minimize this possibility, the revisited version of MetaNSUE presented in this work includes a leave-one-out protection within the MLE step, which consists in iteratively discarding the study that increases the most the MLE effect size. Specifically, the MLE is first estimated with all studies but the first, then with all studies but the second, then with all studies but the third, and so on, and only the combination returning the lowest absolute MLE effect size is kept.

When the number of studies is large, this iteration may have to be repeated several times. In each iteration, one additional study is discarded. This process is repeated until the probability that the meta-analysis is falsely positive due to the issue is not higher than 0.05 even in the worst-case scenario (i.e., when all studies with not-statistically significant effect sizes are NSUEs). This probability may be estimated as follows. In a meta-analysis with $N$ studies, the meta-analysis could be falsely positive if there was one falsely positive study and this positive study made the meta-analysis positive. However, the meta-analysis could also be falsely positive if there were two falsely positive studies and these two positive studies made the meta-analysis positive, or if there were three falsely positive studies and these three positive studies made the meta-analysis positive.  In general, the meta-analysis could be falsely positive if there were $k$ falsely positive studies and these $k$ positive studies made the meta-analysis positive. Then, the overall probability is the sum of these probabilities:
\begin{equation}
  \label{eq:psumk1}
  P=\sum _{k=1}^{N}p_{{\rm FP\; studies}} \left(k,N\right)\cdot p_{{\rm FP\; meta}-{\rm analysis}} \left(k,N\right)
\end{equation}
where $p_{FP studies}$ is the probability of having $k$ falsely positive studies, and $p_{FP meta-analysis}$ is the probability of having a falsely positive meta-analysis if there are $k$ falsely positive studies. $p_{FP studies}$ is simply the probability mass function of the binomial distribution and can be thus straightforwardly calculated. Conversely, $p_{FP meta-analysis}$ does not have a simple expression but with simulations, we found that for $\alpha=0.05$ can be approximated as:
\begin{equation}
  p_{{\rm FP\; meta}-{\rm analysis}} \left(k,N\right)=\left(1+\exp \left(\begin{aligned} 5.62 &\hspace{0.5cm}if k=1 \\ -0.82+0.059N &\hspace{0.5cm}if k=2 \\ 1.27+0.13k-0.0075N &\hspace{0.5cm}if k>2 \end{aligned}\right)\right)^{-1}
\end{equation}
Each leave-one-out iteration removes one potential falsely positive study, decreasing $k$, $N$, and the overall probability of having a falsely positive meta-analysis. The strategy to know the optimal number of studies to discard consists then in repeatedly conducting leave-one-out iterations until the overall probability of having a falsely positive meta-analysis is not higher than $\alpha=0.05$. If the number of studies is very large, the number is roughly approximated using a linear simplification.

When the user does not aim to conduct a simple, mean meta-analysis but a meta-regression or other linear models, studies in the leave-one-out iterations must be discarded according to the effect size of the hypothesis of interest, rather than according to the effect size of the mean. For this reason, the user must specify the hypothesis to contrast before conducting the MLE, and a new MLE must be conducted for each other new hypothesis.
\subsubsection[Estimation of heterogeneity]{Estimation of heterogeneity $\tau^2$}
Once the coefficients have been estimated, a second MLE step is conducted to estimate the between-study heterogeneity parameter $\tau^{2}$ using all studies. The likelihood to maximize is the same as in \eqref{eq:loglikelihood}, and we have added the same numerical improvements described in \eqref{eq:logpmf}. However, the $pdf$ and $pmf$ described in \eqref{eq:pdf} and \eqref{eq:pmf} are modified to include $\tau^{2}$ (and to use $e_{i} = y_{i}$ -- $X_{i}\cdot\beta$ instead of $y_{i}$):
\begin{equation}
  \begin{aligned}
  pdf\left(e_{i} ,v_{i}^{2} |\tau ^{2} \right) &= \frac{1}{\sqrt{v_{i}^{2} +\tau ^{2} } } \phi \left(\frac{e_{i} }{\sqrt{v_{i}^{2} +\tau ^{2} } } \right)\\
    &= \frac{1}{\sqrt{v_{i}^{2} +\tau ^{2} } } \cdot \frac{1}{\sqrt{2\pi } } \cdot \exp \left(-\frac{1}{2} \cdot \frac{e_{i}^{2} }{v_{i}^{2} +\tau ^{2} } \right)\\
    -log\left(pdf\left(y_{i} ,v_{i}^{2} |\tau ^{2} \right)\right) &= \frac{1}{2} \cdot \log \left(v_{i}^{2} +\tau ^{2} \right)+\frac{1}{2} \cdot \log \left(2\pi \right)+\frac{1}{2} \cdot \frac{e_{i}^{2} }{v_{i}^{2} +\tau ^{2} }\\
    &= \frac{1}{2} \cdot \log \left(2\pi \right)+\frac{1}{2} \cdot \log \left(v_{i}^{2} +\tau ^{2} \right)+\frac{1}{2} \cdot \frac{e_{i}^{2} }{v_{i}^{2} +\tau ^{2} }\\
    &=(ct.)+\frac{1}{2} \cdot \left[\log \left(v_{i}^{2} +\tau ^{2} \right)+\frac{e_{i}^{2} }{v_{i}^{2} +\tau ^{2} } \right]
    \end{aligned}
    \end{equation}

\begin{equation}
  \begin{aligned}
    pmf\left(e_{{\alpha  \mathord{\left/{\vphantom{\alpha  2}}\right.\kern-\nulldelimiterspace} 2} ,i} ,e_{1-{\alpha  \mathord{\left/{\vphantom{\alpha  2,i}}\right.\kern-\nulldelimiterspace} 2,i} } ;v_{{\alpha  \mathord{\left/{\vphantom{1-\alpha  2,i}}\right.\kern-\nulldelimiterspace} 2,i} }^{2} ,v_{{1-\alpha  \mathord{\left/{\vphantom{\alpha  2,i}}\right.\kern-\nulldelimiterspace} 2,i} }^{2} |\tau ^{2} \right)=\Phi \left(\frac{e_{{1-\alpha  \mathord{\left/{\vphantom{1-\alpha  2,i}}\right.\kern-\nulldelimiterspace} 2,i} } }{\sqrt{v_{{1-\alpha  \mathord{\left/{\vphantom{1-\alpha  2,i}}\right.\kern-\nulldelimiterspace} 2,i} }^{2} +\tau ^{2} } } \right)-\Phi \left(\frac{e_{{\alpha  \mathord{\left/{\vphantom{\alpha  2,i}}\right.\kern-\nulldelimiterspace} 2,i} } }{\sqrt{v_{{\alpha  \mathord{\left/{\vphantom{\alpha  2,i}}\right.\kern-\nulldelimiterspace} 2,i} }^{2} +\tau ^{2} } } \right)
  \end{aligned}
\end{equation}

\subsection{Multiple imputation}
The multiplication of \textit{X} and \textit{$\beta$} returns the expected value of the effect size of each NSUE study. For example, if $\beta = (3.2, 0.2)$ and $X_{i} = (1, 2)$, the expected value of the effect size for this $i^{th}$ study is $1\times3.2+2\times0.2 = 3.6$. Then, the multiple imputation step consists in adding Gaussian noise to the expected value in order to have realistic imputations. In the example, random noise would be added to $3.6$ so that imputation $1$ could be $3.3$, imputation $2$ could be $3.7$, and etcetera. Note that imputing the NSUE studies using their $expected$ value without added noise, would erroneously mean assuming that both within-study variability and between-study heterogeneity are null.

The addition of noise must meet two conditions: a) the noise must be the sum of the variance of the effect size plus $\tau^{2}$; and b) the noisy imputation must be between the effect size bounds. The truncated normal distribution accommodates these conditions, and thus each imputation is a random generation of:

\begin{equation}
  \label{eq:yi}
pdf\left(y_{i} |\theta \right)=\frac{\frac{1}{\sqrt{v_{i}^{2} +\hat{\tau }^{2} } } \cdot \phi \left(\frac{y_{i} -\hat{y}_{i} }{\sqrt{v_{i}^{2} +\hat{\tau }^{2} } } \right)}{\Phi \left(\frac{y_{i,1-{\alpha  \mathord{\left/{\vphantom{\alpha  2}}\right.\kern-\nulldelimiterspace} 2} } -\hat{y}_{i} }{\sqrt{v_{i}^{2} +\hat{\tau }^{2} } } \right)-\Phi \left(\frac{y_{i,{\alpha  \mathord{\left/{\vphantom{\alpha  2}}\right.\kern-\nulldelimiterspace} 2} } -\hat{y}_{i} }{\sqrt{v_{i}^{2} +\hat{\tau }^{2} } } \right)}, y_{i} \in \left(y_{{i,\alpha  \mathord{\left/{\vphantom{i,\alpha  2}}\right.\kern-\nulldelimiterspace} 2} }, y_{i,1-{\alpha  \mathord{\left/{\vphantom{\alpha  2}}\right.\kern-\nulldelimiterspace} 2} } \right)
\end{equation}

However, it must be noted that for some measures such as standardized mean differences, the variance depends on the imputed effect sizes (rather than on the sample size alone). Therefore, in those imputations in which the noise decreases the effect size, the variance is smaller and the study receives more weight. Conversely, in those imputations in which the noise increases the effect size, the variance is larger and the study receives less weight. In order to avoid this bias, the probability of a given effect size is weighted by the inverse of the weight (i.e., the variance) that the effect size would receive in a subsequent meta-analysis:
\begin{equation}
  \label{eq:yimeta}
  pdf\left(y_{i} |\theta \right)=\frac{\left(\hat{v}_{i,y_{i} }^{2} +\hat{\tau }^{2} \right)\cdot \frac{1}{\sqrt{\hat{v}_{i,\hat{y}_{i} }^{2} +\hat{\tau }^{2} } } \cdot \phi \left(\frac{y_{i} -\hat{y}_{i} }{\sqrt{\hat{v}_{i,\hat{y}_{i} }^{2} +\hat{\tau }^{2} } } \right)}{\bigint _{x=y_{{i,\alpha  \mathord{\left/{\vphantom{i,\alpha  2}}\right.\kern-\nulldelimiterspace} 2} } }^{y_{{i,1-\alpha  \mathord{\left/{\vphantom{i,1-\alpha  2}}\right.\kern-\nulldelimiterspace} 2} } }{\left(\left(\hat{v}_{i,x}^{2} +\hat{\tau }^{2} \right)\cdot \frac{1}{\sqrt{\hat{v}_{i,\hat{y}_{i} }^{2} +\hat{\tau }^{2} } } \cdot \phi \left(\frac{x-\hat{y}_{i} }{\sqrt{\hat{v}_{i,\hat{y}_{i} }^{2} +\hat{\tau }^{2} } } \right)\right)} }, y_{i} \in \left(y_{{\alpha  \mathord{\left/{\vphantom{\alpha  2}}\right.\kern-\nulldelimiterspace} 2} } ,y_{1-{\alpha  \mathord{\left/{\vphantom{\alpha  2}}\right.\kern-\nulldelimiterspace} 2} } \right)
\end{equation}
In practice, the continuum between the bounds is divided in a number of bins; the probability of each bin is calculated using the numerator of \eqref{eq:yimeta}, and a bin is randomly selected according to its probability.

Also important, the noise for related studies is created accounting for the correlations between them. Specifically, instead of straightforwardly using the expected values and variances as in \eqref{eq:yi} or \eqref{eq:yimeta}, these are conditioned to the observed and/or to the already imputed related studies (\cite{RN2}). For example, for two related studies, the expected value and variance of the first study are simply as in \eqref{eq:yi} or \eqref{eq:yimeta}, whereas the expected value and variance of the second study are:

\begin{equation}
  \begin{aligned}
    \left(\begin{array}{c} {\hat{y}_{2,1}^{*} } \\ {\vdots } \\ {\hat{y}_{2,n_{imp} }^{*} } \end{array}\right) &= \left(\begin{array}{c} {y_{1,1} } \\ {\vdots } \\ {y_{1,n_{imp} } } \end{array}\right)\times \hat{\gamma } \\
    \hat{v}_{2}^{*^{2}} &=\hat{v}_{2}^{2} -\hat{\gamma }^\top \times \hat{\Pi }_{XX}\times \hat{\gamma }
    \end{aligned}
\end{equation}

where $y_{1,j}$ is the $j^{th}$ imputation of the study 1, $n_{imp}$ is the number of imputations, and $\hat{\gamma}$ and $\hat{\Pi}_{XX}$ are the coefficients and covariance matrix of the linear model used to predict the expected values and variances of the study $2$ from the imputations of study $1$:

\begin{equation}
  \begin{aligned}
    \hat{\gamma }&=\hat{\Pi }_{XX}^{-1} \times \hat{\Pi }_{XY}\\
    \hat{\Pi }_{XX} &=\left(\hat{v}_{1}^{2} +\hat{\tau }^{2} \right) \\
    \hat{\Pi }_{XY} &=\left(\sqrt{\hat{v}_{1}^{2} +\hat{\tau }^{2} } \right)\cdot \sqrt{\hat{v}_{2}^{2} +\hat{\tau }^{2} } \cdot r
  \end{aligned}  
\end{equation}

\subsection{Meta-analysis, pooling and hypothesis testing}
After the multiple imputation step, there are several datasets of complete data. For each imputed dataset, related studies are combined so that each group of related studies is converted into a single study. The effect size of this ``combined study'' is simply the mean effect size of the corresponding related studies, whereas the variance of the ``combined study'' is the mean variance of the corresponding related studies divided by the sum of the weights calculated in \eqref{eq:weightrelated}. In other words, the variance of the ``combined study'' is reduced by (\cite{RN8}\cite{RN9}\cite{RN10}):
\begin{equation}
    \sum _{i=1}^{R_{i} }w_{i}  =\sum _{i=}^{R_{i} }\frac{1}{1+\left(R_{i} -1\right)\cdot r}  =\frac{R_{i} }{1+\left(R_{i} -1\right)\cdot r}
\end{equation}
The resulting datasets can be meta-analyzed using standard formula. As in the ``metafor'' package \cite{RN5}, MetaNSUE estimates $\tau^{2}$ using restricted-maximum likelihood (REML) for its statistical advantages (\cite{RN11}), and the covariates included in the MLE step are also included here in the modeling to allow testing meta-regressions or other linear models:
\begin{equation}
  \begin{aligned}
    \beta &=\Lambda \times X^\top \times W\times Y\\
    \Lambda &=\left(X^\top \times W\times X\right)^{-1}\\
    H^{2} &=1+\frac{\hat{\tau }_{REML}^{2} }{df} \cdot trace\left(P_{FE} \right)\\
    I^{2} &=1-\frac{1}{H^{2} } ,{\rm \; \; \; \; \; }I^{2} \ge 0\\
    Q&=Y^\top \times P_{FE} \times Y,{\rm \; \; \; \; \; }Q\ge 0
    \end{aligned}
\end{equation}
where $\beta$ are the coefficients, $\Lambda$ is their covariance matrix, $H^{2}$, $I^{2}$ and $Q$ are heterogeneity statistics, and:
\begin{equation}
  \begin{aligned}
    W&=diag\left({\raise0.7ex\hbox{$ 1 $}\!\mathord{\left/{\vphantom{1 \hat{v}_{i}^{2} +\hat{\tau }_{REML}^{2} }}\right.\kern-\nulldelimiterspace}\!\lower0.7ex\hbox{$ \hat{v}_{i}^{2} +\hat{\tau }_{REML}^{2}  $}} ,...\right)\\
    P_{FE} &=W_{FE} -W_{FE} \times X\times \left(X^\top \times W_{FE} \times X\right)^{-1} \times X^\top \times W_{FE} \\
    W_{FE} &=diag\left({\raise0.7ex\hbox{$ 1 $}\!\mathord{\left/{\vphantom{1 \hat{v}_{i}^{2} }}\right.\kern-\nulldelimiterspace}\!\lower0.7ex\hbox{$ \hat{v}_{i}^{2}  $}} ,...\right)
  \end{aligned}
\end{equation}
After a meta-analysis has been conducted for each imputation dataset, the resulting statistics are pooled \cite{RN6} in order to have a single vector of coefficients and a single covariance matrix:
\begin{equation}
  \begin{aligned}
    \beta _{pooled} &=mean\left(\beta \right)\\
    \sigma _{pooled}^{2} &=mean\left(\sigma ^{2} \right)+\left(1+\frac{1}{n_{imp} } \right)\cdot var\left(\sigma ^{2} \right)
  \end{aligned}
\end{equation}
Finally, the hypothesis specified by the user is tested using the pooled statistics:
\begin{equation}
  \begin{aligned}
    \beta _{H} &=H\times \beta _{pooled} \\
    \sigma _{H}^{2} &=H\times \Lambda _{pooled} \times H^\top \\
    Z&=\frac{\beta _{H} }{\sigma _{H} }
  \end{aligned}
\end{equation}
As commented earlier, if the user wants to test another hypothesis, the process must be repeated from the MLE step, typing the ``meta'' command again.
\subsection{Example of use}
<<prompt = "R> ", continue = "+  ", width = 70, useFancyQuotes = FALSE>>=
t <- c(3.4, NA, NA, NA, NA, 2.8, 2.1, 3.1, 2.0, 3.4)
n <- c(40, 20, 22, 24, 18, 30, 25, 30, 16, 22)
x <- smc_from_t(t, n)
m <- meta(x)
m
str(m)
@

\section{Other functions in the package}
\subsection{forest}
This function creates a forest plot of the studies, showing their effect sizes and the 95\% confidence intervals. For studies with NSUE, a light gray shadow shows the interval containing 95\% of the imputations. The function also returns the optimal width and height of the plot.
\subsubsection{Example of use}
<<prompt = "R> ", continue = "+  ", width = 70, useFancyQuotes = FALSE>>=
t <- c(3.4, NA, NA, NA, NA, 2.8, 2.1, 3.1, 2.0, 3.4)
n <- c(40, 20, 22, 24, 18, 30, 25, 30, 16, 22)
x <- smc_from_t(t, n)
m <- meta(x)
forest(m)
@
See an example of outcome in Figure \ref{fig:forestfunnel}
\subsection{funnel}
This function creates a funnel plot of the studies, showing the residual effect size of the studies on the x-axis and their standard errors on the y-axis. For studies with NSUE, a light gray shadow shows the ellipse containing 95\% of the imputations.

Asymmetry in the funnel plot is usually understood as an indication of publication bias, but this might not be always the case and the researcher should be especially cautious because there are other causes of asymmetry, such as selective outcome reporting, poor methodological quality in the smaller studies, or true heterogeneity (i.e., effect size truly depends on study size).

\subsubsection{Example of use}
<<prompt = "R> ", continue = "+  ", width = 70, useFancyQuotes = FALSE>>=
t <- c(3.4, NA, NA, NA, NA, 2.8, 2.1, 3.1, 2.0, 3.4)
n <- c(40, 20, 22, 24, 18, 30, 25, 30, 16, 22)
x <- smc_from_t(t, n)
m <- meta(x)
funnel(m)
@
See an example of outcome in Figure \ref{fig:forestfunnel}

\subsection{leave1out}
This function is analogue to ``meta'' but, instead of meta-analyzing all the studies once, it meta-analyses all the studies but one n times. This is useful for exploring how each individual study affects the overall estimate of the rest of the studies.
\subsubsection{Example of use}
<<prompt = "R> ", continue = "+  ", width = 70, useFancyQuotes = FALSE>>=
t <- c(3.4, NA, NA, NA, NA, 2.8, 2.1, 3.1, 2.0, 3.4)
n <- c(40, 20, 22, 24, 18, 30, 25, 30, 16, 22)
x <- smc_from_t(t, n)
summary(leave1out(x))
@

\subsection{metabias}
This function conducts a meta-regression of the effect sizes of the studies by their standard errors. This aims to detect asymmetry in the funnel plot indicating potential publication bias, and consequently the researcher should take the same caution than with funnel plots.

\subsubsection{Example of use}
<<prompt = "R> ", continue = "+  ", width = 70, useFancyQuotes = FALSE>>=
t <- c(3.4, NA, NA, NA, NA, 2.8, 2.1, 3.1, 2.0, 3.4)
n <- c(40, 20, 22, 24, 18, 30, 25, 30, 16, 22)
x <- smc_from_t(t, n)
m <- meta(x)
metabias(m)
@

\subsection{subset}
This function returns a subset of the studies included in a ``nsue'' object, according to a logical condition.
\subsubsection{Example of use}
<<prompt = "R> ", continue = "+  ", width = 70, useFancyQuotes = FALSE>>=
t <- c(3.4, NA, NA, NA, NA, 2.8, 2.1, 3.1, 2.0, 3.4)
n <- c(40, 20, 22, 24, 18, 30, 25, 30, 16, 22)
x <- smc_from_t(t, n)
x
sx <- subset(x, n > 20)
sx
@

\section{The GUI}
The main window of the GUI shows a table interface, which offers functionalities for entering and modifying the data. Rows correspond to studies and columns to variables. In a meta-analysis of one-sample studies, there must be a column called ``study'' with the labels of the studies, a column called ``n1'' with their sample sizes, and a column containing the outcome statistic (e.g., the t-value of each study). In a meta-analysis of two-sample studies, the column called ``n1'' must specify the sizes of the first group (e.g., the patients), and there must be another column called ``n2'' with the sample sizes of the second group (e.g., the controls). Figure \ref{fig:mainwindow} displays the main window with the buttons for managing data tables (New, Load and Save), as well as buttons for adding and deleting studies and/or variables.
\begin{figure}[h]
  \centering
  \includegraphics{images/edit_cell_linux.png}
  \caption{Main window of the GUI with an example data table loaded.}\label{fig:mainwindow}
\end{figure}


After saving the table, the user must press the button ``Run MetaNSUE'', at the bottom right of the main window. A dialog asks then the type of outcome (e.g., a correlation), the name of the variable specifying the outcome statistic (e.g., ``t''), and whether the user wants to conduct a main analysis or a meta-regression. In case of a main analysis, a second dialog will ask which additional analyses and/or plots must be conducted. In case of a meta-regression, the user must select the modulators (and optionally specify a user-defined hypothesis matrix) (Figure \ref{fig:dialogs}).
\begin{figure}[h]
  \centering
  \includegraphics{images/mean_lm_dialog_linux.png}
  \caption{Dialogs for Mean (left) and Linear Model (right) analyses.}\label{fig:dialogs}
\end{figure}

After pressing the button 'Launch MetaNSUE', a webpage with the results of the meta-analysis will be automatically open (see Section \ref{sec:workedexamples} for examples).

\section{Worked examples}
\label{sec:workedexamples}

\subsection{Main analysis}
The file ``example1.txt'' contains simulated data of the differences in the blood levels of phosphate between patients with a given disorder and healthy controls. Within the GUI, press the button ``Load table'' and select the file ``example1.txt'' to load the data, which are organized in the following columns:
\begin{itemize}
\item{``study''}: specifies a label for each study (here we used the name of the first author and the year of publication of the study),
\item{``n1''}: specifies the number of patients of each study,
\item{``n2''}: specifies the number of controls of each study,
\item{``t''}: specifies the t statistic of the t-test conducted in each study to compare the blood levels of phosphate between patients and controls. 
\end{itemize}
Thus, the study in the first row was conducted by Tintin et al on 2004, included $43$ patients and $44$ controls, and the $t$ statistic of its $t$-test was $2.35$. The paper of the study in the second row, conducted by Milu et al on 2013, reports that it did not find statistically significant differences between patients and controls, but it does not report the actual value of the $t$ statistic; this is the reason why this is coded as ``NA'' (Not Available).

To conduct the meta-analysis, press the button ``Run MetaNSUE'', select ``Standard mean difference (2-sample)'', ``t'' and ``Main analysis (mean)'', press the button ``Next'', select optional tests and plots, press the button ``Next'', and press the button ``Launch MetaNSUE''. The GUI will open the web file displayed in Figure \ref{fig:t_mean}.

\begin{figure}[h]
  \centering
  \includegraphics{images/t_mean.png}
  \caption{Results webpage for a meta-analysis of a 2-sample standard mean differences.}\label{fig:t_mean}
\end{figure}

You can click the plots to use larger, high-resolution pictures, like the ones displayed in Figure \ref{fig:forestfunnel}.

\begin{figure}[h]
  \centering
  \includegraphics{images/forest_funnel.png}
  \caption{Forest plot (left) and funnel plot (right).}\label{fig:forestfunnel}
\end{figure}

Find below the code to conduct the same analysis within R:
<<prompt = "R> ", continue = "+  ", width = 70, useFancyQuotes = FALSE>>=
data <- read.table("example1.txt", header = TRUE)
y <- smd_from_t(data$t, data$n1, data$n2, labels = data$study)
m <- meta(y)
m
summary(leave1out(y))
metabias(m)
forest(m)
funnel(m)
@
\subsection{Meta-regression}
The file ``example2.txt''also contains simulated data of the differences in the blood levels of phosphate between patients with a given disorder and healthy controls, plus an additional column with the mean age of the participants of each study.

To know whether the differences between patients and controls are modulated by the age of the participants, press the button ``Run MetaNSUE'', select ``Standard mean difference (2-sample)'', ``t'' and ``Meta-regression (linear model)'', press the button ``Next'', select ``age'', press the button ``Next'', and press the button ``Launch MetaNSUE''. The GUI will open the web file displayed in Figure \ref{fig:t_lm_age}.
\begin{figure}[h]
  \centering
  \includegraphics{images/t_lm_age.png}
  \caption{Results webpage for a meta-regression of a 2-sample standard mean differences using the variable ``age'' as a modulator.}\label{fig:t_lm_age}
\end{figure}

Find below the code to conduct the same analysis within R:
<<prompt = "R> ", continue = "+  ", width = 70, useFancyQuotes = FALSE>>=
data <- read.table("example2.txt", header = TRUE)
y <- smd_from_t(data$t, data$n1, data$n2, labels = data$study)
meta(y, ~ data$age)
@

\subsection{Tables without missing data}
The files ``example1\_truth.txt'' and ``example2\_truth.txt'' contain the same data, but the NSUEs have been replaced by the actual t statistics obtained during the simulation of the studies. The user may repeat the analyses and find that the results are similar to those above, and identical to the ones obtained with the ``metafor'' package \cite{RN5}.
<<prompt = "R> ", continue = "+  ", width = 70, useFancyQuotes = FALSE>>=
data <- read.table("example1_truth.txt", header = TRUE)
y <- smd_from_t(data$t, data$n1, data$n2, labels = data$study)
meta(y)

library(metafor)
n <- nrow(data)
y <- escalc(measure = "SMD", vtype = "UB",
           m1i = data$t * sqrt(1 / data$n1 + 1 / data$n2),
           n1i = data$n1, n2i = data$n2,
           m2i = rep(0, n), sd1i = rep(1, n), sd2i = rep(1, n))
rma(y)
data <- read.table("example2_truth.txt", header = TRUE)
y <- smd_from_t(data$t, data$n1, data$n2, labels = data$study)
meta(y, ~ data$age)
n <- nrow(data)
y <- escalc(measure = "SMD", vtype = "UB",
           m1i = data$t * sqrt(1 / data$n1 + 1 / data$n2),
           n1i = data$n1, n2i = data$n2,
           m2i = rep(0, n), sd1i = rep(1, n), sd2i = rep(1, n))
rma(y, mods = data$age)
@

\section{Discussion}
This paper presents an improved version of MetaNSUE, a novel meta-analytic software that can include studies that report that the outcome was not statistically significant but do not report the actual outcome. This version includes numerical improvements, making the software more robust in extreme scenarios, as well as a new, easy-to-use GUI. With these improvements, we hope that both \proglang{R} users and non-\proglang{R} users are able to conduct robust MetaNSUE meta-analyses rather than, incorrectly, excluding NSUE studies or assuming that they have a null effect size.

\smallskip

\section*{Acknowledgements}

We would like to thank Dr. Roby Joehanes from Harvard Medical School for providing valuable help in some of the numerical improvements explained in the text. 

\bibliography{references}
\end{document}
